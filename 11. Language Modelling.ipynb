{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Problem of Modeling Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formal languages like programming languages can be fully specified. All the reserved words can be defined and the valid ways they can be used can be precisely defined. We cannot do this with Natural Language. Natural Languages are not designed; they emerge, and therfore there is no formal specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Statistical Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.\n",
    "\n",
    "A Language model learns the probability of word occurence based in examples of text. Simpler models may look at a context of a short sequence of words, whereas larger models may work at the level of sentences or paragraphs. Most commonly, language models operate at the level of words.\n",
    "\n",
    "Basically used on the front-end or back-end of a more sophisticated model for a task that requires language understanding.\n",
    "\n",
    "A good example is speech recognition, where audio data is used as an input signal and recognizes each new word within the context of the words already recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are used to generate text in many similar natural language processing tasks, for example:\n",
    "    \n",
    "    1. Optical Character Recognition.\n",
    "    2. Handwriting Recognition.\n",
    "    3. Machine Translation.\n",
    "    4. Spelling Correction.\n",
    "    5. Image Captioning.\n",
    "    6. Text Summarization.\n",
    "    7. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new methods have sown significant progress over statistical methods. Secifically, a word embedding is adopted that uses a real-valued vector to represent each word in a projected space. This learned representation of words based on their usage allows words with a similar meaning to have a similar representation.\n",
    "\n",
    "This generalization learned through the embeddings is something that the representation used in classical statistical language models cannot easily achieve.\n",
    "\n",
    "Furthermore, the distributed representation approach allows the embedding representation to scale better with the size of the vocabulary. Classical methods that have one discrete representation per word fight the curse of dimentionalitywith larger and larger vocabularies of words that result in longer and more sparse representations.\n",
    "\n",
    "The neural network approach to language modeling can be described using the three following model properties:\n",
    "    \n",
    "    1. Associate each word in the vocabulary with a distributed word feature vector.\n",
    "    2. Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence.\n",
    "    3. Learn Simultaneously the word feature vector and the parameters of the probability function.\n",
    "    \n",
    "This then represents a relatively simple model where both the representation and probabilistic model are learned together directly from raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural network models outperform statistical models, here are some heuristics for developing high-performing neural language models in general:\n",
    "    \n",
    "    1. Size matters. The best models are the largest models, specifically number of memory units.\n",
    "    2. Regularization matters. use of regularization like dropout in input connections improves results.\n",
    "    3. CNNs vs Embeddings. Character-level Convolutional Neural Network (CNN) models can be used on the front-end instead of word embeddings, achieving similar and sometimes better results.\n",
    "    4. Ensembles matter. Combining the prediction from multiple models can offer large improvements in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
