{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful with great progress in Language modeling and Document Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The problem with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with text is its always messy, and ML algorithms prefer well defined fixed-length inputs and outputs. Since machine learning algorithms cannot work with raw text directly, the text must be converted into numbers. Specifically, vectors of numbers. This process is called feature extraction or feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is a Bag-of-Words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW is a way of extracting features from text for use in modeling. It is a representation of text that describes the occurence of words within a document, and involves two things:\n",
    "    \n",
    "    1. A vocabulary of known words.\n",
    "    2. A measure of the presence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is called a bag of words because any information about order or structure of words in the document is discarded. The model is only concerned about known words occuring *in* the document, not *where* in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOW can be as simple or complex as you'd want it to be. The complexity comes both in deciding how to design the vocabulary of known words (or tokens) and how to score the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Example of the Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Step 1: Collect Data\n",
    "    \n",
    "    Collect the data as sentences or a document.\n",
    "    \n",
    "    \n",
    "3.2. Step 2: Design the Vocabulary\n",
    "    \n",
    "    Make a list or set of all words in the vocabulary ignoring punctuations and case\n",
    "    \n",
    "\n",
    "3.3. Step 3: Create Document Vectors\n",
    "    \n",
    "    Score the words in each document. Objective being to turn each document of free text into a vector that we can use as input or output for an ML model. Words not encoded in the train document will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Managing Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the vocab size increases, so does the vector representation of documents. In this case what then happens will be a case of a vector with lots of zeros called a sparse vector or sparce representation.\n",
    "\n",
    "Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms. Thus there now is the pressure to reduce the size of the vocabulary when using a BOW model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple cleaning steps that can be used as a first step:\n",
    "    \n",
    "    1. Ignoring case.\n",
    "    2. Ignoring punctuation.\n",
    "    3. Ignoring frequent words that dont contain much information, called stop words.\n",
    "    4. Fixing misspelled words.\n",
    "    5. Reducing words to their stem using stemming algorithms.\n",
    "    \n",
    "A sophisticated approach would be to create a vocabulary of grouped words. This changes the scope of the vocabulary and allows the BOW to capture a bit more meaning from the document. In this method, each word or token is called a *gram*.\n",
    "Creating a vocab of two-word pairs is then called a *bi-gram* model.\n",
    "\n",
    "Again, only the bi-grams that appear in the corpus are modeled, not all possible bi-grams.\n",
    "A vocab that tracks triplets of words is called a trigram model and the general approach is called the n-gram model, where **n** refers to the number of grouped words. Often, a simple bigram approach is better than a 1-gram BOW model for tasks like document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Scoring Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a vocab has been chosen, the occurence of words in example documents needs to be scored. Some scoring methods include:\n",
    "    \n",
    "    1. Counts: Count the number of times each word appears in a document.\n",
    "    2. Frequencies: Calculate the frequency that each word appears in a document out of all the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Word Hashing:\n",
    "    \n",
    "    **Remember:** A hash function is a bit of math that maps data to a fixed size set of numbers. For example, we use them in hash tables when programming where perhaps names are converted to numbers for fast lookup.\n",
    "    \n",
    "    We can use a hash representation of known words in our vocab. this addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, while is in turn the size of the vector representation of the document.\n",
    "    \n",
    "    Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. This is called the Hash trick or feature hashing. \n",
    "    \n",
    "    The challenge then is in choosing a hash space to accomodate the chosen vocab size to minimize the probability of collisions and trade-off sparsity.\n",
    "    \n",
    "5.2. TF-IDF:\n",
    "    \n",
    "    A problem with scoring word frequencies is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much informational content to the model as rarer but perhaps domain specific words. One approach is to rescale the frequency of words by how often they apprea in all documents, so that the scores for frequent words like *the* that are also frequent across all documents are penalized. This approach is called Term Frequency - Inverse Document Frequency, or TF-IDF for short, where:\n",
    "    \n",
    "    Term Frequency: is a scoring of the frequency of the words in the current document.\n",
    "    Inverse Document Frequency: is a scoring of how rare the word is across documents. \n",
    "    \n",
    "The scores then are a weighting where not all words are equally as important or interesting. The scores have the effect of highlighting words that are distinct (contain useful information) in a given document.\n",
    "\n",
    "*Thus the IDF of a rare term is high, whereas the IDF of a frquent term is likely to be low.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Limitations of Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model though quite robust and simple to understand, suffers from a few shortcomings such as:\n",
    "    \n",
    "    1. Vocabulary: The vocab requires careful design, most specifically in order to manage the size, which impacts the sparcity of the document representations.\n",
    "        \n",
    "    2. Sparcity: Sparce representations are harder to model both for computational reasons (space and time complexity) and also for informational reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "        \n",
    "    3. Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modelled could tell the difference between the same words differently arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
