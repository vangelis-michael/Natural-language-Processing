{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They ae a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What are Word Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a learned representation for text where words that have the same meaning have asimilar representation. It works successfully with natural language processing problems.\n",
    "\n",
    "It is a class of techniques where individual words are represented as real-valued vectors in a predefined vector space.\n",
    "\n",
    "Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "Key to this approach is the idea of using a dense distributed representation for each word. Each word is represented by a real-valued vector, often tens or hundreds of dimensions. This is contrasted to the thousands or millions of dimensions required for sparse word representations, such as one hot encoding.\n",
    "\n",
    "The distributed representation is learned based on the usage of words. This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning. This can be contrasted with the crisp but fragile representations, regardless of how they are used.\n",
    "\n",
    "*You shall know a word by the company it keeps!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word Embedding Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text. \n",
    "\n",
    "The learning is either joint with the neural net model on some tasks (document classification) or an unsupervised process, using document statistics.\n",
    "\n",
    "We will look at three techniques that can be used to learn a word embedding from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as language modeling or document classification.\n",
    "\n",
    "It requires that document be cleaned and prepared such that each word is one hot encoded. The size of the vector space is then specified as part of the model, such as 50, 100, 300 dimensions.\n",
    "\n",
    "The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm.\n",
    "\n",
    "The one hot encoded words are mapped to the word vectors. If an MLP model is used, then the word vectors are concatenated before being fed as input to the model.\n",
    "If a recurrent network is used, then each word may be taken as one input in a sequence. This approach of learning an embedding requires a lot of training data and can be slow, but will learn an embedding both targeted to the specific text data and the NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical method for efficiently learning a standalone word embedding from a text corpus. It captures high syntactic and semantic regularities in language.\n",
    "\n",
    "Two learning models were introduced that can be used as part of the Word2Vec approach to learn the word embedding:\n",
    "    \n",
    "    1. Continuous Bag-of-Words. CBOW\n",
    "    2. Continuous Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1. Continuous Bag-of-Words.\n",
    "\n",
    "The CBOW learns the embedding by predicting the current word based on its context.\n",
    "\n",
    "2.2.2. Continuous Skip-Gram Model\n",
    "\n",
    "The continuous skip-gram learns the embedding by predicting the surrounding words given a current word.\n",
    "\n",
    "\n",
    "**Both models are focused on learning about words given their local usage context, where the context si defined by a window of neighboring words. This window is a configurable parameter of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key benefit of this approach is the high-quality embeddings that can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora or text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Vectors for Word representation algorithm is an extension of the Word2Vec method for efficiently learning word vectors. \n",
    "\n",
    "Classical vector space model representations of words were developed using matrix factorization techniques such as Latent Semantic Analysis (LSA) that do a good job of using global text statistics but are not as good as the learned methods like Word2Vec at capturing meaning and demonstrating it on tasks like calculating analogies.\n",
    "\n",
    "GloVe is an approach that tries to combine the global statistics of matrix factorization techniques like LSA with the local context-based learning in Word2Vec. Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurence matrix using statistics across the whole text corpus.\n",
    "\n",
    "The result is then a learning model that may result in generally better word embeddings.\n",
    "\n",
    "Useful in word analogy, word similarity, and named entity recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use word embeddings, we need to do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. learn an Embedding.\n",
    "\n",
    "This will require a large amount of text data to ensure that useful embeddings are learned. We have two options:\n",
    "\n",
    "    1. Learn it Standalone, where a model is trained to learn the embedding, which is saved and used as a part of another model for a later task. This is a good approach if you will like to use the same embedding in multiple models.\n",
    "    2. Learn Jointly, where the embedding is learned as part of a large task-specific model. This is a good approach if you only intend to use the embedding on one task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Reuse an Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to use pre-trained word embeddings for your projects. For example Word2Vec and GloVe word embeddings are available for free. These can be used instead of training your own embeddings from scratch. You have two options when it comes to using pre-trained embeddings:\n",
    "\n",
    "    1. Static: Where the embedding is kept static and is used as a component of your model. This is suitable if the embedding is a good fit for your problem and gives good results.\n",
    "        \n",
    "    2. Updated: Where the pre-trained embedding is used to seed the model, but the embedding is updated jointly during the training of the model. this may be a good option if you are looking to get the most out of the model and embedding on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
