{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Models for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word Embeddings + CNN = Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification invoves the use of a word embedding for representing words and a Convolutional Neural Network (CNN) for learning how to discriminate documents on classification problems.\n",
    "\n",
    "ConvNets are effective at document classification namely because they are able to pick out salient features (tokens, or word sequences) in a way that is invariant to their position within the input sequences.\n",
    "\n",
    "The architecture is thus described as:\n",
    "    \n",
    "    1. Word Embedding. A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.\n",
    "    2. Convolutional Model. A feature extraction model that learns to extract salient features from documents represented using a word embedding.\n",
    "    3. Fully-Connected Model. The interpretation of extracted features in terms of a predictive output.\n",
    "    \n",
    "     **We know the CNN is a feature-extracting architecture, meant to be integrated into a larger network and be trained to work in tendem in order to produce an end result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a Single Layer CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN text classification architecture from the paper by Yoon Kim:\n",
    "\n",
    "    He discovered that using pre-trained word vectors for classification tasks does very well. Suggesting we use pre-trained embeddings that are trained on very large text corpora.\n",
    "    \n",
    "    Transfer function: Rectified Linear.\n",
    "    Kernel Sizes: 2, 4, 5\n",
    "    Number of Filters: 100\n",
    "    Dropout rate: 0.5\n",
    "    Weight regularization (L2): 3\n",
    "    Batch size: 50\n",
    "    Update Rule: Adadelta\n",
    "    \n",
    "These configurations can be used to inspire a starting point for your own experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dial in CNN Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Convolutional Neural Networks for Sentence classification, one can take the following findings from Ye Zhang and Byron Wallace as steps in the right direction towards building a model.\n",
    "\n",
    "    1. The choice of pretrained Word2Vec and GloVe embeddings differ from problem to problem, and both performed better than using one hot encoded word vectors.\n",
    "    2. the size of the kernel is important and should be tuned for each problem.\n",
    "    3. The number of feature maps is also important and should be tuned.\n",
    "    4. The 1-max pooling generally outperformed other types of pooling.\n",
    "    5. Dropout has little effect on the model performance.\n",
    "    \n",
    "Specific Heuristics:\n",
    "    \n",
    "    1. Use Word2Vec or GloVe word embeddings as a starting point and tune them while fitting the model.\n",
    "    2. Grid search across different kernel sizes to find the optimal configuration for your problem, in the range 1 -10.\n",
    "    3. Search the number of filters from 100-600 and explore a dropout of 0.0-0.5 as part of the same search.\n",
    "    4. Explore using tanh, relu, amd linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Consider Character-Level CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text documents can be modeled at the character level using convnets that are capable of learning the relevant hierarchical structure of words, sentences, paragraphs, and more.\n",
    "\n",
    "The promise of this method is, all the effort required to prepare text could be overcome if a CNN can learn to abstract the salient details.\n",
    "\n",
    "In this method, \n",
    "    \n",
    "    *The model reads in one hot encoded characters in a fixed-size alphabet. \n",
    "    *Encoded characters are read in blocks or sequences of 1024 characters.\n",
    "    *A stack of 6 convolutional layers with pooling follows, with 3 fully connected layers at the output end of the network in order to make a prediction.\n",
    "    \n",
    "This method performs better on problems that offer a larger corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Consider Deeper CNNs for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better performance, it is hoped can be achieved with deeper nets than seen above or in the NLP text by Jason Brownlee.\n",
    "\n",
    "Although in most Neural NLP models, only about 5 - 6 layers have been used, a heavy contrast to the computer vision tasks that have sometimes up to 152 layers. This suggests that NLP challenges can afford to try out very deep systems and get rewards much better than shallow nets.\n",
    "\n",
    "This will be called VDCNN - Very Deep Convolutional Neural Networks. And the key to their approach will be embedding of individual characters, rather than a word embedding. We can note from below:\n",
    "    * The very deep architecture worked well on small and large datasets.\n",
    "    * Deeper networks decrease classification error.\n",
    "    * Maxpooling achieves better results than other, more sophisticated types of pooling.\n",
    "    * Generally going deeper degrades accuracy; the shortcut connections used in the architecture are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
